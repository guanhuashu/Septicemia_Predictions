{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BMI707_text_preprocessing_newest.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"QTldrv08El6p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619126638897,"user_tz":600,"elapsed":20859,"user":{"displayName":"Taylor Shishido","photoUrl":"","userId":"14908051122747758659"}},"outputId":"889f7b7f-cdad-451f-de2c-f8583061eae5"},"source":["import numpy as np\n","import pandas as pd\n","from google.colab import files\n","from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"evmaZQM7dw54"},"source":["###Side task: split training set into training and validation"]},{"cell_type":"code","metadata":{"id":"jnxXPeCVd35c","executionInfo":{"status":"ok","timestamp":1619126917760,"user_tz":600,"elapsed":9321,"user":{"displayName":"Taylor Shishido","photoUrl":"","userId":"14908051122747758659"}}},"source":["# open file and read the content in a list\n","train_notes_cleaned=[]\n","with open('gdrive/MyDrive/BMI707_Final_Project/train_notes_cleaned.txt', 'r') as f1:\n","    for line in f1:\n","        # remove linebreak which is the last character of the string\n","        note = line[:-1]\n","\n","        # add item to the list\n","        train_notes_cleaned.append(note)        \n","\n","#read in continuous outcomes\n","train_data_balanced = pd.read_csv(\"gdrive/MyDrive/BMI707_Final_Project/train_data_balanced.csv\")\n","train_labels_ordinal = np.array(train_data_balanced.outcome_count) \n","train_labels_binary = np.array(train_data_balanced.binary_outcome)      "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"IV-LxrlkfAKx","executionInfo":{"status":"ok","timestamp":1619127582880,"user_tz":600,"elapsed":5681,"user":{"displayName":"Taylor Shishido","photoUrl":"","userId":"14908051122747758659"}}},"source":["#attach binary outcome, outcome count, and ID to padded_notes_trainNEW.csv\n","\n","padded_notes_trainNEW = pd.read_csv(\"gdrive/MyDrive/BMI707_Final_Project/padded_notes_trainNEW.csv\")\n","padded_notes_trainNEW[\"outcome_count\"] = train_data_balanced.outcome_count\n","padded_notes_trainNEW[\"ID\"] = train_data_balanced.ID\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_BnAityhjgI","executionInfo":{"status":"ok","timestamp":1619128070104,"user_tz":600,"elapsed":647,"user":{"displayName":"Taylor Shishido","photoUrl":"","userId":"14908051122747758659"}}},"source":["#attach binary outcome, outcome count, and ID to train_notes_cleaned\n","type(train_notes_cleaned)\n","train_notes_cleaned_df = pd.DataFrame(train_notes_cleaned)\n","train_notes_cleaned_df[\"outcome_count\"] = train_data_balanced.outcome_count\n","train_notes_cleaned_df[\"binary_count\"] = train_data_balanced.binary_outcome\n","train_notes_cleaned_df[\"ID\"] = train_data_balanced.ID"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bT8sjppfUJS","executionInfo":{"status":"ok","timestamp":1619128075387,"user_tz":600,"elapsed":641,"user":{"displayName":"Taylor Shishido","photoUrl":"","userId":"14908051122747758659"}},"outputId":"5db0b2c2-ea98-4e93-9cac-643446997ec5"},"source":["import random\n","new_data = train_data_balanced\n","#split ids by binary outcome, since test and train should be balanced\n","new_data_outcome0 = new_data[new_data.binary_outcome == 0]\n","new_data_outcome1 = new_data[new_data.binary_outcome == 1]\n","\n","#get unique ids for each outcome\n","patient_ids_outcome0 = new_data_outcome0.ID.unique()\n","print(len(patient_ids_outcome0))\n","patient_ids_outcome1 = new_data_outcome1.ID.unique()\n","print(len(patient_ids_outcome1))\n","\n","\n","#shuffle each id list\n","random.Random(4).shuffle(patient_ids_outcome0)\n","random.Random(4).shuffle(patient_ids_outcome1)\n","\n","#split ids  in EACH list\n","total_ids = len(patient_ids_outcome0) #get total number of unique ids\n","split = round(total_ids*.8) #find the 80% split index for training\n","train_split_ids_outcome0 = patient_ids_outcome0[:split]\n","val_split_ids_outcome0 = patient_ids_outcome0[split:]\n","\n","total_ids = len(patient_ids_outcome1) #get total number of unique ids\n","split = round(total_ids*.8) #find the 80% split index for training\n","train_split_ids_outcome1 = patient_ids_outcome1[:split]\n","val_split_ids_outcome1 = patient_ids_outcome1[split:]\n","\n","#recombine balanced patients  to make full train and test\n","train_split_ids = np.concatenate((train_split_ids_outcome0, train_split_ids_outcome1), axis=0)\n","val_split_ids = np.concatenate((val_split_ids_outcome0, val_split_ids_outcome1), axis=0)\n","\n","\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["9943\n","4586\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BZDzLIFIf8MT","executionInfo":{"status":"ok","timestamp":1619128078048,"user_tz":600,"elapsed":1285,"user":{"displayName":"Taylor Shishido","photoUrl":"","userId":"14908051122747758659"}}},"source":["#split full dataset, train_notes_cleaned_df,  and padded_notes_trainNEW based on ids\n","train_data_full_dataset = new_data.loc[new_data['ID'].isin(train_split_ids)]\n","val_data_full_dataset = new_data.loc[new_data['ID'].isin(val_split_ids)]\n","\n","train_data_notes_cleaned = train_notes_cleaned_df.loc[train_notes_cleaned_df['ID'].isin(train_split_ids)]\n","val_data_notes_cleaned = train_notes_cleaned_df.loc[train_notes_cleaned_df['ID'].isin(val_split_ids)]\n","\n","train_data_padded = padded_notes_trainNEW.loc[padded_notes_trainNEW['ID'].isin(train_split_ids)]\n","val_data_padded = padded_notes_trainNEW.loc[padded_notes_trainNEW['ID'].isin(val_split_ids)]\n","\n"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54m10a_ZjFA2","executionInfo":{"status":"ok","timestamp":1619128272599,"user_tz":600,"elapsed":682,"user":{"displayName":"Taylor Shishido","photoUrl":"","userId":"14908051122747758659"}},"outputId":"ac864b04-a58a-4426-b9b9-be3687adcd52"},"source":["print(val_data_padded.shape)\n","print(train_data_notes_cleaned.shape)\n","train_data_full_dataset.shape"],"execution_count":33,"outputs":[{"output_type":"stream","text":["(26056, 516)\n","(105635, 4)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(105635, 12)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"34ECBFqHiwub","executionInfo":{"status":"ok","timestamp":1619128306647,"user_tz":600,"elapsed":31014,"user":{"displayName":"Taylor Shishido","photoUrl":"","userId":"14908051122747758659"}}},"source":["#write data sets to cv\n","train_data_full_dataset.to_csv('gdrive/MyDrive/BMI707_Final_Project/TRAINING_full_dataset.csv',index=False)\n","val_data_full_dataset.to_csv('gdrive/MyDrive/BMI707_Final_Project/VALIDATION_full_dataset.csv',index=False)\n","\n","train_data_notes_cleaned.to_csv('gdrive/MyDrive/BMI707_Final_Project/TRAINING_notes_cleaned.csv',index=False)\n","val_data_notes_cleaned.to_csv('gdrive/MyDrive/BMI707_Final_Project/VALDIATION_notes_cleaned.csv',index=False)\n","\n","train_data_padded.to_csv('gdrive/MyDrive/BMI707_Final_Project/TRAINING_notes_padded.csv',index=False)\n","val_data_padded.to_csv('gdrive/MyDrive/BMI707_Final_Project/VALDIATION_notes_padded.csv',index=False)\n"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5BxG2Iqbd1sj"},"source":["###End of side task"]},{"cell_type":"code","metadata":{"id":"xg6xeYWOXGOk","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":70},"outputId":"13ea5c89-6632-4d9d-c1e1-f7aab1048d97"},"source":["#read in data\n","uploaded = files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-195b42a0-7cf1-4b63-8fac-025e13f1519b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-195b42a0-7cf1-4b63-8fac-025e13f1519b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving joined_notes_outcome_patient_data.csv to joined_notes_outcome_patient_data (1).csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Eu37q3gcZEuc"},"source":["import io\n","data = pd.read_csv(io.BytesIO(uploaded['joined_notes_outcome_patient_data.csv']))\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIDesWNMWWw4"},"source":["#select relevant columns\n","notes = data[[\"TEXT\"]]\n","binary_labels = data[[\"binary_outcome\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phvgIrRkh9HG"},"source":["Split into test and train datasets. We need to do this before becaue the Tokenizer should ONLY be generated on the training data\n"]},{"cell_type":"code","metadata":{"id":"UHaqbdIyhnzJ"},"source":["#create a new ID system so dont have to continue using two columns\n","\n","#merge the 2 ids in new column\n","data[\"ID\"] = data.SUBJECT_ID.astype(str) + \"-\" + data.HADM_ID.astype(str)\n","data.head()\n","\n","#remove old ids\n","new_data = data.drop([\"SUBJECT_ID\", \"HADM_ID\"], axis=1)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBkUKunhRBcx"},"source":["import random\n","\n","#split ids by binary outcome, since test and train should be balanced\n","new_data_outcome0 = new_data[new_data.binary_outcome == 0]\n","new_data_outcome1 = new_data[new_data.binary_outcome == 1]\n","\n","#get unique ids for each outcome\n","patient_ids_outcome0 = new_data_outcome0.ID.unique()\n","print(len(patient_ids_outcome0))\n","patient_ids_outcome1 = new_data_outcome1.ID.unique()\n","print(len(patient_ids_outcome1))\n","\n","\n","#shuffle each id list\n","random.Random(4).shuffle(patient_ids_outcome0)\n","random.Random(4).shuffle(patient_ids_outcome1)\n","\n","#split ids  in EACH list\n","total_ids = len(patient_ids_outcome0) #get total number of unique ids\n","split = round(total_ids*.8) #find the 80% split index for training\n","train_ids_outcome0 = patient_ids_outcome0[:split]\n","test_ids_outcome0 = patient_ids_outcome0[split:]\n","\n","total_ids = len(patient_ids_outcome1) #get total number of unique ids\n","split = round(total_ids*.8) #find the 80% split index for training\n","train_ids_outcome1 = patient_ids_outcome1[:split]\n","test_ids_outcome1 = patient_ids_outcome1[split:]\n","\n","#recombine balanced patients  to make full train and test\n","train_ids = np.concatenate((train_ids_outcome0, train_ids_outcome1), axis=0)\n","test_ids = np.concatenate((test_ids_outcome0, test_ids_outcome1), axis=0)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wP2Zn8NFTXIK"},"source":["#check lists\n","print(len(train_ids))\n","len(test_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zozdikIi-nB"},"source":["#split full dataset based on ids\n","train_data = new_data.loc[new_data['ID'].isin(train_ids)]\n","test_data = new_data.loc[new_data['ID'].isin(test_ids)]\n","\n","#split into input (notes) and output (labels)\n","train_notes = train_data.TEXT\n","train_labels = train_data.binary_outcome\n","test_notes = test_data.TEXT\n","test_labels = test_data.binary_outcome"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5HEBSvt9m5t2"},"source":["#check number of notes for train and test\n","print(str(len(train_notes)) + ' notes in training data which has 104 patients') # how did we know it is 104?\n","print(str(len(test_notes)) + ' notes in test data which has 27 patients')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WeMjJpReRGiq"},"source":["##Download datasets"]},{"cell_type":"code","metadata":{"id":"wFxlquxLRFjp"},"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","#write data sets to cv\n","train_data.to_csv('/content/gdrive/My Drive/train_data_full.csv',index=False)\n","test_data.to_csv('/content/gdrive/My Drive/test_data_full.csv',index=False)\n","\n","files.download('/content/gdrive/My Drive/train_data_full.csv')\n","files.download('/content/gdrive/My Drive/test_data_full.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7HsZDBd7pzw"},"source":["##Reload train/test datasets here (can run full notebook from here)\n"]},{"cell_type":"code","metadata":{"id":"tFD5tBO75oMa"},"source":["#RELOAD DATASSETS\n","\n","# from google.colab import files\n","# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","\n","# !ls\n","\n","# train_data = pd.read_csv('/content/gdrive/My Drive/bmi707/train_data_balanced.csv')\n","# test_data = pd.read_csv('/content/gdrive/My Drive/bmi707/test_data_balanced.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SxPuuUE86ZSf"},"source":["#split into input (notes) and output (labels)\n","train_notes = train_data.TEXT\n","train_labels = train_data.binary_outcome\n","test_notes = test_data.TEXT\n","test_labels = test_data.binary_outcome\n","\n","#check number of notes for train and test\n","print(str(len(train_notes)) + ' notes in training data which has 104 patients')\n","print(str(len(test_notes)) + ' notes in test data which has 27 patients')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8XxP6wX2c7PK"},"source":["##Sources: \n","\n","https://medium.com/@nwamaka_41565/predicting-hospital-readmission-using-nlp-5f0fe6f1a705\n","\n","##ClinicalBert requires minimal preprocessing: (excerpt from above article)\n","\n","First, words are converted to lowercase\n","\n","Line breaks are removed\n","\n","Carriage returns are removed\n","\n","De-identified the personally identifiable info inside the brackets\n","\n","Remove special characters like ==, −−\n","\n","The SpaCy sentence segmentation package is used to segment each note . i think this is implemented automatically with NLTK.\n","\n","##Other Notes: ((excerpt from above article)\n","\n","Since clinical notes don’t follow rigid standard language grammar, we find rule-based segmentation has better results than dependency parsing-based segmentation. Various segmentation signs that misguide rule-based segmentators are removed or replaced.\n","\n","For example 1.2 would be removed.\n","\n","M.D., dr. would be replaced with with MD, Dr\n","\n","Clinical notes can include various lab results and medications that also contain numerous rule-based separators, such as 20mg, p.o., q.d.. (where q.d. means one a day and q.o. means to take by mouth. To address this, segmentations that have less than 20 words are fused into the previous segmentation so that they are not singled out as different sentences.\n"]},{"cell_type":"code","metadata":{"id":"9r18udB1fwoz"},"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","import re\n","\n","from nltk import pos_tag\n","from nltk.stem import WordNetLemmatizer # lemmatizes word based on its parts of speech\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFfUqCZjgNpn"},"source":["Look at stop words and punctuation that will be removed"]},{"cell_type":"code","metadata":{"id":"j-2zOUE5f7fs"},"source":["nltk.download('stopwords')\n","print('Punctuation:', string.punctuation)\n","print('NLTK English Stop Words:', '\\n', stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"05M1GCcUgkx_"},"source":["Convert POS tags from string package to what is used in WordNet (NLTK) package. all from the article"]},{"cell_type":"code","metadata":{"id":"5rtoZa_EgSy6"},"source":["def convert_tag(treebank_tag):\n","    '''Convert Treebank tags to WordNet tags'''\n","    if treebank_tag.startswith('J'):\n","        return 'a'\n","    elif treebank_tag.startswith('V'):\n","        return 'v'\n","    elif treebank_tag.startswith('N'):\n","        return 'n'\n","    elif treebank_tag.startswith('R'):\n","        return 'r'\n","    else:\n","        return 'n' # if no match, default to noun"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1VIQjIYIDL5d"},"source":["Define lemmatizer (stemming) and preprocess_and_tokenize. these were all from the article! "]},{"cell_type":"code","metadata":{"id":"uFi9AJGNgS2-"},"source":["def lemmatizer(tokens):\n","    '''\n","    Performs lemmatization.\n","    Params:\n","        tokens (list of strings): cleaned tokens with stopwords removed\n","    Returns:\n","        lemma_words (list of strings): lemmatized words\n","    '''  \n","    # POS-tag your data before lemmatizing\n","    tagged_words = pos_tag(tokens) # outputs list of tuples [('recent', 'JJ'),...]\n","    \n","    \n","    # Lemmatize using WordNet's built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet.\n","    wnl = WordNetLemmatizer()\n","    \n","    lemma_words = []\n","    \n","    # Lemmatize list of tuples, output a list of strings\n","    for tupl in tagged_words:\n","        lemma_words.append(wnl.lemmatize(tupl[0], convert_tag(tupl[1])))\n","    \n","    return lemma_words\n","def preprocess_and_tokenize(text):\n","    '''\n","    Clean the data.\n","    Params:\n","        text (string): full original, uncleaned text\n","    Returns:\n","        lemmatized_tokens (list of strings): cleaned words\n","    '''\n","    # Make text lowercase\n","    text = text.lower()\n","    \n","    # Remove punctuation\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) \n","    \n","    # Remove numbers and words that contain numbers\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    \n","    # Remove newline chars and carriage returns\n","    text = re.sub('\\n', '', text)\n","    text = re.sub('\\r', '', text)\n","    \n","    # Tokenize\n","    word_tokens = word_tokenize(text) \n","    \n","    # Remove stop words\n","    tokens = [word for word in word_tokens if word not in stopwords.words('english')]\n","# Call lemmatizer function above to perform lemmatization\n","    lemmatized_tokens = lemmatizer(tokens)\n","    \n","    return lemmatized_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbOHafdlpqOK"},"source":["#manually run preprocess_and_tokenize on every note BOTH train and test data\n","\n","#additional nltk packages needed \n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","\n","\n","cleaned_train_notes = [preprocess_and_tokenize(x) for x in train_notes]  \n","cleaned_test_notes = [preprocess_and_tokenize(x) for x in test_notes]  \n","\n","#less efficient ways of ^^\n","# cleaned_train_notes = []\n","# for i in train_notes:\n","#   note = preprocess_and_tokenize(i)\n","#   cleaned_train_notes.append(note)\n","# cleaned_test_notes = []\n","# for i in test_notes:\n","#   note = preprocess_and_tokenize(i)\n","#   cleaned_test_notes.append(note)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5xSfsPuR5oY"},"source":["##Download cleaned train and test notes"]},{"cell_type":"code","metadata":{"id":"BHf7YueNRrit"},"source":["# write cleaned tokenized notes to text file\n","with open('/content/gdrive/My Drive/train_notes_cleaned_full.txt', 'w') as f:\n","  for item in cleaned_train_notes:\n","        f.write(\"%s\\n\" % item)\n","with open('/content/gdrive/My Drive/test_notes_cleaned_full.txt', 'w') as f:\n","  for item in cleaned_test_notes:\n","        f.write(\"%s\\n\" % item)\n","\n","files.download('/content/gdrive/My Drive/train_notes_cleaned_full.txt')\n","files.download('/content/gdrive/My Drive/test_notes_cleaned_full.txt')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DyzvW1plGEJ"},"source":["print(cleaned_test_notes[1:3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xHS3zgClv6IT"},"source":["##Convert each note from word sequence to integer vector\n","\n","This is equivalent to text_to_sequences function in BMI 707 code. I could not do that bc I had to make a customized tokenize function and could not use the default keras Tokenizer class.\n"]},{"cell_type":"code","metadata":{"id":"5dJq8ZU5SxTh"},"source":["#rejoin tokenized words from preprocess_and_tokenize back into strings\n","separator=' '\n","rebuilt_train_notes = [separator.join(i) for i in cleaned_train_notes]   \n","rebuilt_test_notes = [separator.join(i) for i in cleaned_test_notes]    \n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","#create keras tokenizer\n","max_words = 10000\n","tokenizer = Tokenizer(max_words, oov_token='OOV')\n","tokenizer.fit_on_texts(rebuilt_train_notes)\n","vocab_size = max_words + 1\n","\n","#convert text to sequences\n","cleaned_train_notesNEW = tokenizer.texts_to_sequences(rebuilt_train_notes)\n","cleaned_test_notesNEW = tokenizer.texts_to_sequences(rebuilt_test_notes)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8kQ5p-SXDQP"},"source":["new_train_seqs = cleaned_train_notesNEW\n","new_test_seqs = cleaned_test_notesNEW\n","\n","seq_len = 512\n","padded_notes_trainNEW = pad_sequences(new_train_seqs, maxlen=seq_len)\n","padded_notes_testNEW = pad_sequences(new_test_seqs, maxlen=seq_len)\n","print(padded_notes_trainNEW.shape)\n","\n","\n","#write NEW padded notes to text file\n","with open('/content/gdrive/My Drive/padded_notes_trainNEW_full.txt', 'w') as f:\n","  for item in padded_notes_trainNEW:\n","        f.write(\"%s\\n\" % item)\n","with open('/content/gdrive/My Drive/padded_notes_testNEW_full.txt', 'w') as f:\n","  for item in padded_notes_testNEW:\n","        f.write(\"%s\\n\" % item)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NFdWSjNdczoz"},"source":["#Do not pay attention to anything below here\n"]},{"cell_type":"code","metadata":{"id":"-PLCU0R1rHAe"},"source":["#convert each note from word sequence to integer vector\n","import itertools\n","from gensim.corpora.dictionary import Dictionary\n","\n","#create dictionary for training data notes only\n","flat_notes = list(itertools.chain(*cleaned_train_notes))\n","note_vocab = Dictionary([flat_notes])\n","\n","#get size of vocabulary - should be less than 10,000\n","vocab_size = len(np.unique(np.array(flat_notes)))\n","oov_int = vocab_size + 5 #make token for out of vocab\n","\n","#remake integer sequences from tokenized notes for train data\n","train_sequences = []\n","for i in cleaned_train_notes:\n","  \n","  seq = [note_vocab.token2id[token] for token in i]\n","  train_sequences.append(seq)\n","\n","#remake integer sequences from tokenized notes for test data \n","test_sequences = []\n","for i in cleaned_test_notes:\n","  \n","    new_seq = [note_vocab.token2id[token] if token in note_vocab else oov_int for token in i ] \n","\n","  # for j in i:      very slow way\n","  #   new_seq=[]\n","  #   if j in note_vocab:\n","  #     new_seq.append(note_vocab.token2id[j])\n","  #   else:   new_seq.append(oov_int) #append oov index\n","\n","    test_sequences.append(new_seq)  #append full sequence to test_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75b6owVJE-4g"},"source":["##I think we can use the gensim Dictionary called node_vocab to remap integers back into words after output is generated."]},{"cell_type":"code","metadata":{"id":"zujHfnJ2D5xV"},"source":["#get size of vocabulary - should be less than 10,000\n","len(np.unique(np.array(flat_notes)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9mzOb030DyI"},"source":["## Pad sequences and trunate long sequences\n","\n","Next we 0-pad the sequences, and short sequences will have 0s prepended so that each sequence is exactly 100 integers long. Note long sentences will be trimmed so that only the first 100 words are used."]},{"cell_type":"code","metadata":{"id":"iG1iOMnl0IZ4"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","seq_len = 512\n","padded_notes_train = pad_sequences(train_sequences, maxlen=seq_len)\n","padded_notes_test = pad_sequences(test_sequences, maxlen=seq_len)\n","print(padded_notes_train.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1as8Oujv_xpe"},"source":["##Save test and train datasets"]},{"cell_type":"code","metadata":{"id":"rq74g49Q_wz8"},"source":["# leaned_test_notes = [preprocess_and_tokenize(x) for x in test_notes]  \n","\n","#write padded notes to text file\n","with open('/content/gdrive/My Drive/train_note_seqs.txt', 'w') as f:\n","  for item in padded_notes_train:\n","        f.write(\"%s\\n\" % item)\n","with open('/content/gdrive/My Drive/test_note_seqs.txt', 'w') as f:\n","  for item in padded_notes_test:\n","        f.write(\"%s\\n\" % item)\n","\n","files.download('/content/gdrive/My Drive/train_note_seqs.txt')\n","files.download('/content/gdrive/My Drive/test_note_seqs.txt')  \n","\n","# #write original \n","# with open('/content/gdrive/My Drive/train_note_seqs.txt', 'w') as f:\n","#   f.write(padded_notes_train)\n","# with open('/content/gdrive/My Drive/test_note_seqs.txt', 'w') as f:\n","#   f.write(padded_notes_test)\n","\n","\n","\n","# files.download('/content/gdrive/My Drive/train_data_balanced.csv')\n","# files.download('test_data_balanced.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qa8oplHPrHwO"},"source":[""]},{"cell_type":"code","metadata":{"id":"hp09_XRuD3wo"},"source":["#Use this code to count the nubmer of unique words (ie True vocab size). We do not use this for anything else for now, might be used for Word2Vec model\n","\n","#first build CountVectorizer object (counts the number of words )\n","from sklearn.feature_extraction.text import CountVectorizer\n","vect = CountVectorizer(max_features=10000, #set to 10,000 words\n","                       tokenizer=preprocess_and_tokenize)\n","\n","vect.fit(train_notes.values.astype(str))\n","\n","#additional nltk packages needed \n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","#####This is the only part of this that matters\n","\n","#check how many words are in vectorizer (we set max as 10,000)\n","vocab_size = len(vect.vocabulary_)\n","print(vocab_size)\n","\n","# # build tokenizer\n","# tokenizer = vect.build_tokenizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqYywqU0qyKf"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","texts = train_data['Text']\n","test_sentences = test_data['Text']\n","labels = train_data['Label']\n","\n","vocab_size = max_words + 1\n","\n","sequences = token.texts_to_sequences(texts)\n","test_sequences = token.texts_to_sequences(test_sentences)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnenLO1Eg3-0"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","vect = CountVectorizer(max_features=10000, #set to 10,000 words\n","                       tokenizer=preprocess_and_tokenize)\n","\n","#additional nltk packages needed \n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","\n","# I applied .astype(str) to fix the ValueError: np.nan is an invalid document, expected byte or unicode string.xc\n","# create the vectorizer\n","vect.fit(train_notes.values.astype(str))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvsskcvxmfay"},"source":["#check how many words are in vectorizer (we set max as 10,000)\n","print(len(vect.vocabulary_))\n","\n","#check number of train notes\n","train_notes.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTZF4liBlxHa"},"source":["#Actually create a vector by passing the text into the vectorizer to get back counts.\n","\n","note_matrix = vect.transform(train_notes.astype(str))\n","print(note_matrix.shape) #this should be number_of_notes X vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQOekc0ano0w"},"source":["note_matrix"],"execution_count":null,"outputs":[]}]}